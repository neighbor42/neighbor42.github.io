---
layout  : wiki
title   : MoE(Mixture of Experts) 이해하기
summary : 
date    : 2025-08-12 15:57:16 +0900
updated : 2025-08-13 03:27:48 +0900
tag     : 
toc     : true
public  : true
parent  : [[index]]
latex   : false
resource: 5879214B-0AB5-2FA8-2F8D-6F3E060562A3
---
* TOC
{:toc}

## Mixture of Experts(MoE)란?
- LLM의 품질을 향상시키기 위해 여러 개의 하위 모델 또는 서브 모델(전문가/experts)를 활용하는 기술
- 특정 과제에 가장 적합한 전문가를 선택적으로 활용하는 모델 아키텍처를 지칭한다

### 2가지 주요 구성 요소
a. Experts(전문가)
	- 피드포워드 신경망으로 최소 하나 이상 활성화 될 수 있음
	- 특정 도메인(심리학, 생물학 등)에 특화된 것이 아니라 토큰 수준에서 구문 정보(syntactic information)를 학습한다
b. Router(라우터)
	- 게이트 네트워크라고도 불린다
	- LLM의 각 레이어에서 어떤 토큰을 어떤 전문가에게 보낼지 결정한다
	- 주어진 입력에 가장 적합한 전문가를 선택한다

### 작동 원리
기존의 Dense Model(밀집 모델)은 과제 해결 시 모델의 모든 파라미터를 사용한다. 이는 모든 구성원이 회의에 참여하는 것과 같아 비효율적이다.(모든 연산 자원을 활용 > 비효율적)

반면 MoE는 Sparse Activation 방식을 사용한다. 이는 주어진 과제에 가장 적합하다고 판단되는 일부 전문가의 파라미터만 선택적으로 활성화하여 문제를 해결하는 방식이다. 이를 통해 훨씬 적은 연산량으로 밀집 모델과 동등하거나 그 이상의 성능을 달성하는 것을 목표로 한다. 

---

## 작업 처리 과정
MoE의 작업 처리 과정은 다음과 같다

1. router의 전문가 선택
	- 입력 데이터가 주어지면 라우터는 각 전문가가 이 문제를 얼마나 잘 해결할지에 대한 확률 점수를 계산한다.
2. 상위 전문가 활성화
	- 라우터는 계산된 점수를 바탕으로 가장 높은 점수를 받은 소수의 전문가를 선택하여 활성화한다
3. 결과 종합
	- 활성화된 전문가들이 각자의 결과물을 내놓으면, 라우터는 최초에 계산한 확률 점수를 가중치로 사용하여 이 결과들을 종합하고 최종 답변을 생성한다.

### Load Balancing
- MoE 훈련시 한 가지 장애물은, 일부 experts가 다른 experts보다 빠르게 학습하여 동일한 experts가 너무 자주 선택될 수 있다는 것이다.

#### KeepTop K
이 방법은 훈련 가능한 가우시안 노이즈를 도입해, 동일한 전문가가 항상 선택되는 것을 방지하고 다른 experts가 훈련될 수 있도록 한다. 상위 K개의 전문가를 제외한 모든 전문가의 가중치를 음의 무한대로 설정해 희소성을 도입하고, 결과적으로 소프트맥스 이후 해당 전문가의 확률이 0이 되도록 한다.

#### Auxiliary Loss(Load balancing loss)
훈련 중에 추가되는 이 손실 구성 요소는 expert 간의 동등한 중요도를 촉진한다. Expert 중요도 점수의 coefficient of variation(CV)를 계산하고 이를 낮추는 것을 목표로 한다

#### Expert Capacity
적은 수의 token을 받는 expert의 훈련 부족을 방지하기 위해 expert가 처리할 수 있는 token의 양에 제한을 둔다. 이 임계값을 초과하는 token은 다음으로 가장 가능성 있는 expert에게 라우팅되거나 다음 layer로 전송된다(token overflow)

---

## MoE 계산 요구 사항
- MoE 모델은 많은 수의 sparse parameters(로드할 총 매개변수)를 가지고 있지만 추론 중에는 더 적은 수의 active parameters를 활성화하므로 계산적으로 효율적이다
- 예시로 Mixtral 8x7B(8 experts X 7B 크기)는 총 470억개의 매개변수를 가지고 있지만 추론 중에는 약 130억개만 활성화된다
- 이게 MoE의 주요 강점이다. 사이즈가 클지라도, 추론은 훨씬 더 빠르다.

---

## 비전 모델에서의 MoE
이 영상은 feed-forward 

